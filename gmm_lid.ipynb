{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F25qTV4ubyia",
        "outputId": "9a2b25c5-f987-4987-ff09-277e25da5aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount='True')\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B71xypRIcBPk",
        "outputId": "bc65654b-e607-4dad-8501-7adf54e85484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NVdFP6c9b69b"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import scipy.io.wavfile as wave\n",
        "from scipy.fftpack import fft, ifft, fftshift\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "from sklearn import mixture\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z249lo2De1ad"
      },
      "source": [
        "# ZFCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L0M6qvi7Zds3"
      },
      "outputs": [],
      "source": [
        "def ZFCC ( signal, sr, n_zfcc, frame_length, hop_length) :\n",
        "\n",
        "        signal = np.diff ( signal )\n",
        "        signal = np.cumsum ( np.cumsum ( signal )  )\n",
        "        kernel = np.ones ( frame_length ) / frame_length\n",
        "        signal -=  np.convolve ( signal , kernel , mode = \"same\")\n",
        "        zfcc = librosa.feature.mfcc(y=signal, sr=sr, hop_length=hop_length, n_mfcc=n_zfcc,win_length=int(0.05*sr))\n",
        "        #print(zfcc.shape)\n",
        "        return zfcc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqcKV5eRe3aq"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zHptFw0Gb7Cq"
      },
      "outputs": [],
      "source": [
        "def training(train_data_path,feat_train_path,trained_model_path, n_mix, max_it, n_mfcc, inc_mfcc,inc_mfccd, inc_mfccdd, inc_zfcc, feat_red_method):\n",
        "\n",
        "  all_languages=glob.glob(train_data_path+'*')\n",
        "  print(all_languages)\n",
        "\n",
        "  directory=feat_train_path\n",
        "  if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "      \n",
        "  directory=trained_model_path\n",
        "  if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "  feat_red = []\n",
        "\n",
        "  for itr1 in range(0,len(all_languages)):\n",
        "\n",
        "      wavs=glob.glob(all_languages[itr1]+'/*/*.wav')\n",
        "      \n",
        "      lang=(all_languages[itr1]).split(\"/\")[-1]\n",
        "      \n",
        "      if not os.path.exists(directory):\n",
        "          os.makedirs(directory)\n",
        "\n",
        "      flag = [inc_mfcc, inc_mfccd, inc_mfccdd, inc_zfcc]\n",
        "      count = 0\n",
        "      for i in range(4):\n",
        "        if flag[i] == True:\n",
        "          count = count+1\n",
        "      final_feat=np.empty([0, n_mfcc*count])\n",
        "      for itr2 in range(0,len(wavs)):\n",
        "          \n",
        "          y, srr = librosa.load(wavs[itr2])\n",
        "          y = librosa.resample(y, srr, sr)\n",
        "          # sr=8000\n",
        "          \n",
        "          # hop_length=int(0.005*sr)\n",
        "          mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=n_mfcc,win_length=int(0.05*sr))\n",
        "          mfcc_delta = librosa.feature.delta(mfcc)\n",
        "          mfcc_ddelta = librosa.feature.delta(mfcc_delta)\n",
        "          zfcc = ZFCC(y, sr, n_mfcc, 2*hop_length, hop_length)\n",
        "\n",
        "          if(mfcc.shape[1] != zfcc.shape[1]):\n",
        "            zfcc = np.concatenate((zfcc,np.zeros((n_mfcc,1))),axis=1)\n",
        "\n",
        "          if inc_zfcc == False and inc_mfcc == True and inc_mfccd == True and inc_mfccdd == True: #1\n",
        "            feat = np.concatenate((mfcc, mfcc_delta, mfcc_ddelta),axis=0)\n",
        "          elif inc_zfcc == False and inc_mfcc == True and inc_mfccd == True and inc_mfccdd == False: #2\n",
        "            feat = np.concatenate((mfcc, mfcc_delta),axis=0)\n",
        "          elif inc_zfcc == False and inc_mfcc == True and inc_mfccd == False and inc_mfccdd == True: #3\n",
        "            feat = np.concatenate((mfcc, mfcc_ddelta),axis=0)\n",
        "          elif inc_zfcc == False and inc_mfcc == False and inc_mfccd == True and inc_mfccdd == True: #4\n",
        "            feat = np.concatenate((mfcc_delta, mfcc_ddelta),axis=0)\n",
        "          elif inc_zfcc == False and inc_mfcc == True and inc_mfccd == False and inc_mfccdd == False: #5\n",
        "            feat = mfcc\n",
        "          elif inc_zfcc == False and inc_mfcc == False and inc_mfccd == False and inc_mfccdd == True: #6\n",
        "            feat = mfcc_ddelta\n",
        "          elif inc_zfcc == False and inc_mfcc == False and inc_mfccd == True and inc_mfccdd == False: #7\n",
        "            feat = mfcc_delta\n",
        "          elif inc_zfcc == True and inc_mfcc == True and inc_mfccd == True and inc_mfccdd == True: #8\n",
        "            feat = np.concatenate((mfcc, mfcc_delta, mfcc_ddelta, zfcc),axis=0)\n",
        "          elif inc_zfcc == True and inc_mfcc == True and inc_mfccd == True and inc_mfccdd == False: #9\n",
        "            feat = np.concatenate((mfcc, mfcc_delta, zfcc),axis=0)\n",
        "          elif inc_zfcc == True and inc_mfcc == True and inc_mfccd == False and inc_mfccdd == True: #10\n",
        "            feat = np.concatenate((mfcc, mfcc_ddelta, zfcc),axis=0)\n",
        "          elif inc_zfcc == True and inc_mfcc == False and inc_mfccd == True and inc_mfccdd == True: #11\n",
        "            feat = np.concatenate((mfcc_delta, mfcc_ddelta, zfcc),axis=0)\n",
        "          elif inc_zfcc == True and inc_mfcc == True and inc_mfccd == False and inc_mfccdd == False: #12\n",
        "            feat = np.concatenate((mfcc, zfcc),axis=0)\n",
        "          elif inc_zfcc == True and inc_mfcc == False and inc_mfccd == False and inc_mfccdd == True: #13\n",
        "            feat = np.concatenate((mfcc_ddelta, zfcc),axis=0)\n",
        "          elif inc_zfcc == True and inc_mfcc == False and inc_mfccd == True and inc_mfccdd == False: #14\n",
        "            feat = np.concatenate((mfcc_delta, zfcc),axis=0)\n",
        "          elif inc_zfcc == True and inc_mfcc == False and inc_mfccd == False and inc_mfccdd == False: #15\n",
        "            feat = zfcc\n",
        "          \n",
        "          \n",
        "\n",
        "          feat = feat.transpose()\n",
        "          final_feat=np.concatenate((final_feat,feat),axis=0)\n",
        "          \n",
        "          \n",
        "\n",
        "      if feat_red_method == \"pca\":\n",
        "        feat_red.append(PCA(n_components = n_mfcc))\n",
        "        feat_red[itr1].fit(final_feat)\n",
        "        final_feat = feat_red[itr1].transform(final_feat)\n",
        "      elif feat_red_method == \"trunc_svd\":\n",
        "        feat_red.append(TruncatedSVD(n_components = n_mfcc-1))\n",
        "        feat_red[itr1].fit(final_feat)\n",
        "        final_feat = feat_red[itr1].transform(final_feat)\n",
        "\n",
        "          #print(final_feat.shape)\n",
        "      print(lang)    \n",
        "      #np.savetxt(feat_train_path+lang+\"_all_features.txt\", final_feat, delimiter=\",\")\n",
        "\n",
        "      try:\n",
        "          #gmm = mixture.GaussianMixture(n_components=n_mixtures, covariance_type='diag' , max_iter = max_iterations ).fit(final_feat)\n",
        "          gmm = mixture.GaussianMixture(n_components=n_mix, covariance_type='diag', max_iter = max_it, init_params = 'kmeans').fit(final_feat)\n",
        "      except:\n",
        "          print(\"ERROR : Error while training model for file \"+lang)\n",
        "          \n",
        "      try:\n",
        "          joblib.dump(gmm,trained_model_path+lang+'.pkl')\n",
        "      except:\n",
        "          print(\"ERROR : Error while saving model for \"+lang)\n",
        "          \n",
        "  \n",
        "  print(\"Training Completed\")\n",
        "  return feat_red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6EuJpm8e57W"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GHjmJwBgb7FR"
      },
      "outputs": [],
      "source": [
        "def testing(test_data_path,feat_test,trained_model_path, n_mfcc, inc_mfcc, inc_mfccd, inc_mfccdd, inc_zfcc, feat_red, apply_feat_red):\n",
        "    # train feature extraction\n",
        "    all_languages=glob.glob(test_data_path+'*')\n",
        "\n",
        "    import os\n",
        "    directory=feat_test\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    languages = { all_languages[k]:k for k in range(len(all_languages)) }\n",
        "\n",
        "    num_test_cases={}\n",
        "    tct={}\n",
        "    for e in languages:\n",
        "        num_test_cases[e.replace(test_data_path,'')]=len(os.listdir(e))-1\n",
        "        tct[e.replace(test_data_path,'')]=0\n",
        "\n",
        "    #print(num_test_cases)\n",
        "\n",
        "    lang_names = { all_languages[k].replace(test_data_path,''):k for k in range(len(all_languages)) }\n",
        "\n",
        "    total_languages=len(num_test_cases)\n",
        "\n",
        "    confusion_matrix = np.zeros((total_languages,total_languages))\n",
        "\n",
        "    for itr1 in range(0,len(all_languages)):\n",
        "        \n",
        "        wavs=glob.glob(all_languages[itr1]+'/*/*.wav')\n",
        "        \n",
        "        lang=(all_languages[itr1]).split(\"/\")[-1]\n",
        "        \n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "        \n",
        "        flag = [inc_mfcc, inc_mfccd, inc_mfccdd, inc_zfcc]\n",
        "        count = 0\n",
        "        for i in range(4):\n",
        "          if flag[i] == True:\n",
        "            count = count+1\n",
        "        final_feat=np.empty([0, n_mfcc*count])\n",
        "        feat = np.empty([0,n_mfcc])\n",
        "        for itr2 in range(0,len(wavs)):\n",
        "            #print(wavs[itr2])\n",
        "            \n",
        "            y, srr = librosa.load(wavs[itr2])\n",
        "            y = librosa.resample(y, srr, sr)\n",
        "            # sr=8000\n",
        "\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=n_mfcc,win_length=int(0.05*sr))\n",
        "            mfcc_delta = librosa.feature.delta(mfcc)\n",
        "            mfcc_ddelta = librosa.feature.delta(mfcc_delta)\n",
        "            zfcc = ZFCC(y, sr, n_mfcc, 2*hop_length, hop_length)\n",
        "\n",
        "            if(mfcc.shape[1] != zfcc.shape[1]):\n",
        "                zfcc = np.concatenate((zfcc,np.zeros((n_mfcc,1))),axis=1)\n",
        "\n",
        "            if inc_zfcc == False and inc_mfcc == True and inc_mfccd == True and inc_mfccdd == True: #1\n",
        "              feat = np.concatenate((mfcc, mfcc_delta, mfcc_ddelta),axis=0)\n",
        "            elif inc_zfcc == False and inc_mfcc == True and inc_mfccd == True and inc_mfccdd == False: #2\n",
        "              feat = np.concatenate((mfcc, mfcc_delta),axis=0)\n",
        "            elif inc_zfcc == False and inc_mfcc == True and inc_mfccd == False and inc_mfccdd == True: #3\n",
        "              feat = np.concatenate((mfcc, mfcc_ddelta),axis=0)\n",
        "            elif inc_zfcc == False and inc_mfcc == False and inc_mfccd == True and inc_mfccdd == True: #4\n",
        "              feat = np.concatenate((mfcc_delta, mfcc_ddelta),axis=0)\n",
        "            elif inc_zfcc == False and inc_mfcc == True and inc_mfccd == False and inc_mfccdd == False: #5\n",
        "              feat = mfcc\n",
        "            elif inc_zfcc == False and inc_mfcc == False and inc_mfccd == False and inc_mfccdd == True: #6\n",
        "              feat = mfcc_ddelta\n",
        "            elif inc_zfcc == False and inc_mfcc == False and inc_mfccd == True and inc_mfccdd == False: #7\n",
        "              feat = mfcc_delta\n",
        "            elif inc_zfcc == True and inc_mfcc == True and inc_mfccd == True and inc_mfccdd == True: #8\n",
        "              feat = np.concatenate((mfcc, mfcc_delta, mfcc_ddelta, zfcc),axis=0)\n",
        "            elif inc_zfcc == True and inc_mfcc == True and inc_mfccd == True and inc_mfccdd == False: #9\n",
        "              feat = np.concatenate((mfcc, mfcc_delta, zfcc),axis=0)\n",
        "            elif inc_zfcc == True and inc_mfcc == True and inc_mfccd == False and inc_mfccdd == True: #10\n",
        "              feat = np.concatenate((mfcc, mfcc_ddelta, zfcc),axis=0)\n",
        "            elif inc_zfcc == True and inc_mfcc == False and inc_mfccd == True and inc_mfccdd == True: #11\n",
        "              feat = np.concatenate((mfcc_delta, mfcc_ddelta, zfcc),axis=0)\n",
        "            elif inc_zfcc == True and inc_mfcc == True and inc_mfccd == False and inc_mfccdd == False: #12\n",
        "              feat = np.concatenate((mfcc, zfcc),axis=0)\n",
        "            elif inc_zfcc == True and inc_mfcc == False and inc_mfccd == False and inc_mfccdd == True: #13\n",
        "              feat = np.concatenate((mfcc_ddelta, zfcc),axis=0)\n",
        "            elif inc_zfcc == True and inc_mfcc == False and inc_mfccd == True and inc_mfccdd == False: #14\n",
        "              feat = np.concatenate((mfcc_delta, zfcc),axis=0)\n",
        "            elif inc_zfcc == True and inc_mfcc == False and inc_mfccd == False and inc_mfccdd == False: #15\n",
        "              feat = zfcc\n",
        "\n",
        "            feat = feat.transpose()\n",
        "            final_feat=np.concatenate((final_feat,feat),axis=0)\n",
        "\n",
        "            if apply_feat_red == True:\n",
        "              feat = feat_red[itr1].transform(feat)\n",
        "\n",
        "            #print(final_feat.shape)\n",
        "            max_score=-np.inf\n",
        "            max_lang_name=\"\"\n",
        "            \n",
        "            for modelfile in sorted(glob.glob(trained_model_path+'*.pkl')):\n",
        "                gmm = joblib.load(modelfile) \n",
        "                score=gmm.score(feat)\n",
        "                #print score\n",
        "                if score>max_score:\n",
        "                    max_score,max_lang_name=score,modelfile.replace(trained_model_path,'').replace('.pkl','')\n",
        "\n",
        "            print(lang+\" -> \"+max_lang_name+(\" Y\" if lang==max_lang_name  else \" N\"))\n",
        "\n",
        "            confusion_matrix[ lang_names[lang] ][lang_names[max_lang_name]]+=1\n",
        "\n",
        "        tct[lang]+=len(wavs)\n",
        "\n",
        "        #print(lang)\n",
        "        #np.savetxt(feat_test+lang+\"_all_features.txt\", feat, delimiter=\",\")\n",
        "        \n",
        "    return tct,confusion_matrix,total_languages\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ3PcksGe9Sp"
      },
      "source": [
        "# File Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tuTj8BLdb7Ka"
      },
      "outputs": [],
      "source": [
        "# All paths should be changed according to your file locations\n",
        "\n",
        "feat='/content/gdrive/MyDrive/clean_dataset/feat/'\n",
        "feat_train='/content/gdrive/MyDrive/clean_dataset/train/'\n",
        "feat_test='/content/gdrive/MyDrive/clean_dataset/test/'\n",
        "trained_model='/content/gdrive/MyDrive/cleany_dataset/train_models/'\n",
        "train_data='/content/gdrive/MyDrive/clean_dataset/traindata/'\n",
        "test_data='/content/gdrive/MyDrive/clean_dataset/testdata/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T7E1CJN2chG9"
      },
      "outputs": [],
      "source": [
        "# for removing existing feature folders, models created\n",
        "if os.path.exists(feat):\n",
        "  !rm -rf feat\n",
        "if os.path.exists(trained_model):\n",
        "  !rm -rf trained_model\n",
        "if os.path.exists(feat_train):\n",
        "  !rm -rf feat_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9flA8tBjeqnI"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E4JbTtLPb7My"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_mixtures = 128\n",
        "max_iterations = 100\n",
        "calc_deltas=True\n",
        "sr=8000\n",
        "hop_length=int(0.005*sr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPkRdWgjEM7N",
        "outputId": "ec00a5f8-0179-47c9-fe41-e2d51ea8590a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/manipuri', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/gujarathi', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/telugu', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/marathi', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/odia', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/bengali', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/assamese']\n"
          ]
        }
      ],
      "source": [
        "n_mixtures = 32\n",
        "max_iterations = 50\n",
        "n_mfcc = 13\n",
        "calc_deltas=True\n",
        "sr=8000\n",
        "hop_length=int(0.005*sr)\n",
        "inc_mfcc = False\n",
        "inc_mfccd = False\n",
        "inc_mfccdd = False\n",
        "inc_zfcc = True\n",
        "feat_red_method = [\"none\", \"pca\", \"trunc_svd\"]\n",
        "feat_red = training(train_data,feat_train,trained_model, n_mixtures, max_iterations, n_mfcc, inc_mfcc, inc_mfccd, inc_mfccdd, inc_zfcc, \"none\")\n",
        "tt,conf_mat,tot_spek = testing(test_data,feat_test,trained_model, n_mfcc, inc_mfcc, inc_mfccd, inc_mfccdd, inc_zfcc, feat_red, False)\n",
        "print(tt)\n",
        "#print(\"MFCC + MFCCD + MFCCDD\")\n",
        "print(\"Confusion Matrix:\\n\",conf_mat)\n",
        "print(\"Accuracy: \",(sum([ conf_mat[i][j] if i==j  else 0 for i in range(tot_spek) for j in range(tot_spek) ] )*100)/float(sum([i for i in tt.values()])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "39BGO_qFTDBN",
        "outputId": "7bdf5689-5ad5-4da9-bd0d-4c29193b2457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/manipuri', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/gujarathi', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/telugu', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/marathi', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/odia', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/bengali', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/assamese']\n",
            "MFCCFalse\n",
            "MFCCDFalse\n",
            "MFCCDDFalse\n",
            "ZFCCTrue\n",
            "Confusion Matrix:\n",
            " [[ 0.  0. 13.  4.  0.  3.  0.]\n",
            " [ 0.  8.  4.  5.  0.  0.  3.]\n",
            " [ 0.  1.  6. 10.  2.  1.  0.]\n",
            " [ 0.  0.  0. 19.  1.  0.  0.]\n",
            " [ 0.  2.  4.  7.  6.  1.  0.]\n",
            " [ 0.  0.  6.  7.  1.  6.  0.]\n",
            " [ 2.  0.  5.  5.  1.  3.  4.]]\n",
            "Accuracy:  35.0\n",
            "['/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/manipuri', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/gujarathi', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/telugu', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/marathi', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/odia', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/bengali', '/content/gdrive/MyDrive/enhanced_factory_dataset/traindata/assamese']\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-591e83d16bec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0;31m#feat_red = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           \u001b[0mfeat_red\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeat_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mixtures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_mfcc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_mfccd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_mfccdd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_zfcc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m           \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconf_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtot_spek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeat_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_mfcc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_mfccd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_mfccdd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_zfcc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_red\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0;31m#print(tt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-a57435155340>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(train_data_path, feat_train_path, trained_model_path, n_mix, max_it, n_mfcc, inc_mfcc, inc_mfccd, inc_mfccdd, inc_zfcc, feat_red_method)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitr2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m           \u001b[0;31m# sr=8000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/resampy/core.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         )\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/gufunc.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_ufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_mixtures = 16\n",
        "max_iterations = 25\n",
        "n_mfcc = 13\n",
        "calc_deltas=True\n",
        "sr=8000\n",
        "hop_length=int(0.005*sr)\n",
        "inc_mfcc = [False, True]\n",
        "inc_mfccd = [False, True]\n",
        "inc_mfccdd = [False, True]\n",
        "inc_zfcc = [False, True]\n",
        "feat_red_method = [\"none\", \"pca\", \"trunc_svd\"]\n",
        "\n",
        "for i in range(2):\n",
        "  for j in range(2):\n",
        "    for k in range(2):\n",
        "      for l in range(2):\n",
        "        if (i != 0 or j!= 0 or k!=0 or l != 0):\n",
        "          #feat_red = []\n",
        "          feat_red = training(train_data,feat_train,trained_model, n_mixtures, max_iterations, n_mfcc, inc_mfcc[i], inc_mfccd[j], inc_mfccdd[k], inc_zfcc[l], \"none\")\n",
        "          tt,conf_mat,tot_spek = testing(test_data,feat_test,trained_model, n_mfcc, inc_mfcc[i], inc_mfccd[j], inc_mfccdd[k], inc_zfcc[l], feat_red, False)\n",
        "          print(tt)\n",
        "          print(\"MFCC\" + str(inc_mfcc[i]))\n",
        "          print(\"MFCCD\" + str(inc_mfccd[j]))\n",
        "          print(\"MFCCDD\" + str(inc_mfccdd[k]))\n",
        "          print(\"ZFCC\" + str(inc_zfcc[l]))\n",
        "          print(\"Confusion Matrix:\\n\",conf_mat)\n",
        "          print(\"Accuracy: \",(sum([ conf_mat[i][j] if i==j  else 0 for i in range(tot_spek) for j in range(tot_spek) ] )*100)/float(sum([i for i in tt.values()])))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
